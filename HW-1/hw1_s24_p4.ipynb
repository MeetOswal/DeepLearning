{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKRRjmhbFcx7"
   },
   "source": [
    "In this problem we will train a neural network from scratch using numpy. In practice, you will never need to do this (you'd just use TensorFlow or PyTorch). But hopefully this will give us a sense of what's happening under the hood. \n",
    "\n",
    "For training/testing, we will use the standard MNIST benchmark consisting of images of handwritten images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Zpyb4xQNu2"
   },
   "source": [
    "In the second demo, we worked with autodiff. Autodiff enables us to implicitly store how to calculate the gradient when we call backward. We implemented some basic operations (addition, multiplication, power, and ReLU). In this homework problem, you will implement backprop for more complicated operations directly. Instead of using autodiff, you will manually compute the gradient of the loss function for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kjvPSnDA4J_w"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "plt.imshow(x_train[0],cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klx9qPmxF9jI"
   },
   "source": [
    "Loading MNIST is the only place where we will use TensorFlow; the rest of the code will be pure numpy.\n",
    "\n",
    "Let us now set up a few helper functions. We will use sigmoid activations for neurons, the softmax activation for the last layer, and the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sdyvaUKoF7ux"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Numerically stable sigmoid function based on\n",
    "  # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "  \n",
    "  x = np.clip(x, -500, 500) # We get an overflow warning without this\n",
    "  \n",
    "  return np.where(\n",
    "    x >= 0,\n",
    "    1 / (1 + np.exp(-x)),\n",
    "    np.exp(x) / (1 + np.exp(x))\n",
    "  )\n",
    "\n",
    "def dsigmoid(x): # Derivative of sigmoid\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "  # Numerically stable softmax based on (same source as sigmoid)\n",
    "  # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "  b = x.max()\n",
    "  y = np.exp(x - b)\n",
    "  return y / y.sum()\n",
    "\n",
    "def cross_entropy_loss(y, yHat):\n",
    "  return -np.sum(y * np.log(yHat))\n",
    "\n",
    "def integer_to_one_hot(x, max):\n",
    "  # x: integer to convert to one hot encoding\n",
    "  # max: the size of the one hot encoded array\n",
    "  result = np.zeros(10)\n",
    "  result[x] = 1\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIZEupTHyNM"
   },
   "source": [
    "OK, we are now ready to build and train our model. The input is an image of size 28x28, and the output is one of 10 classes. So, first: \n",
    "\n",
    "Q1. Initialize a 2-hidden layer neural network with 32 neurons in each hidden layer, i.e., your layer sizes should be: \n",
    "\n",
    "784 -> 32 -> 32 -> 10\n",
    "\n",
    "If the layer is $n_{in} \\times n_{out}$ your layer weights should be initialized by sampling from a normal distribution with mean zero and variance 1/$\\max(n_{in},n_{out})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Below Code we initalize weights and biases for the Neural Netowork:\n",
    "-We first define the layer size for each layer\n",
    "-The weights are intialized such that the initalization is from a normal distribution with zero mean and variance of $1/ max(n_{in}, n_{out})$\n",
    "- The size of each weight matrix is (size of input layer, size of output layer) for example: weight matrix which connects input to 1st hidden layer: (32, 784)\n",
    "- The size of each bias matrix is (size of the output layer, 1) for example: bias matrix which connects input to 1st hidden layer: (32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zBeGvbu6FaM_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "Weights shape: (32, 784)\n",
      "Biases shape: (32, 1)\n",
      "\n",
      "Layer 2:\n",
      "Weights shape: (32, 32)\n",
      "Biases shape: (32, 1)\n",
      "\n",
      "Layer 3:\n",
      "Weights shape: (10, 32)\n",
      "Biases shape: (10, 1)\n",
      "\n",
      "Layer: 1\n",
      "(32, 784)\n",
      "Layer: 2\n",
      "(32, 32)\n",
      "Layer: 3\n",
      "(10, 32)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Initialize weights of each layer with a normal distribution of mean 0 and\n",
    "# standard deviation 1/sqrt(n), where n is the number of inputs.\n",
    "# This means the weighted input will be a random variable itself with mean\n",
    "# 0 and standard deviation close to 1 (if biases are initialized as 0, standard\n",
    "# deviation will be exactly 1)\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(80085)\n",
    "# Define the sizes of the layers in the neural network\n",
    "layer_sizes = [784, 32, 32, 10]\n",
    "\n",
    "# Initialize weights and biases for each layer\n",
    "weights = [] # List to store weight matrices for each layer\n",
    "biases = []    # List to store bias vectors for each layer\n",
    "\n",
    "# Loop through each layer (except the output layer)\n",
    "for i in range(len(layer_sizes) - 1):\n",
    "    # Initialize weights\n",
    "    weight_shape = (layer_sizes[i+1], layer_sizes[i])  # Define the shape of the weight matrix\n",
    "    weight_variance = 1.0 / np.sqrt(max(weight_shape))  # He initialization for weights\n",
    "    weight_values = np.random.normal(loc=0.0, scale=weight_variance, size=weight_shape)\n",
    "    weights.append(weight_values)  # Add the weight matrix to the list of weights\n",
    "\n",
    "    # Initialize biases\n",
    "    bias_shape = (layer_sizes[i+1], 1) # Define the shape of the bias vector\n",
    "    bias_values = np.zeros(bias_shape)  # Initialize biases to zero\n",
    "    biases.append(bias_values)  # Add the bias vector to the list of biases\n",
    "\n",
    "# Display the initialized weights and biases\n",
    "for i, (w, b) in enumerate(zip(weights, biases), 1):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(f\"Weights shape: {w.shape}\") # Display the shape of the weight matrix\n",
    "    print(f\"Biases shape: {b.shape}\")  # Display the shape of the bias vector\n",
    "    print()\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    print(f\"Layer: {i + 1}\")\n",
    "    print(weights[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IafUGD_VGeLh"
   },
   "source": [
    "Next, we will set up the forward pass. We will implement this by looping over the layers and successively computing the activations of each layer. \n",
    "\n",
    "Q2. Implement the forward pass for a single sample, and for the entire dataset.\n",
    "\n",
    "\n",
    "Right now, your network weights should be random, so doing a forward pass with the data should not give you any meaningful information. Therefore, in the last line, when you calculate test accuracy, it should be somewhere around 1/10 (i.e., a random guess)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function of one_hot_softmax is a function which takes the ouptput given by the neural network as the input and outputs an array of 0s and 1 which has the max probability in the original input to the function.\n",
    "example:\n",
    "input: [0.2, 0.6, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "output: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_softmax(softmax_output):\n",
    "    \n",
    "    # Create one-hot encoding\n",
    "    max_indices = np.argmax(softmax_output)\n",
    "    one_hot_output = np.zeros_like(softmax_output)\n",
    "    one_hot_output[max_indices] = 1\n",
    "    \n",
    "    return one_hot_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed_forward_sample:\n",
    "This function takes a sample image and its actual label. It flattens the image into a array and inputs it into the neural netowork.\n",
    "The for loop iterates over all the layers of the network and compute values on each layer using previously defined weights and bias and use sigmoid activation function for all layers accept last layer.\n",
    "The last layer has the activation function of softmax.\n",
    "Finaly we calcualte the loss as cross entropy loss and we return the prediction for the given array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Dataset function:\n",
    "a for loop is run over all example and each example is send to the feed forward sample function with its actual label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cd6jGroQGdOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 2.36\n",
      "Accuracy (# of correct guesses): 808.0 / 10000 ( 8.08 %)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def feed_forward_sample(sample, y):\n",
    "  \"\"\" Forward pass through the neural network.\n",
    "    Inputs:\n",
    "      sample: 1D numpy array. The input sample (an MNIST digit).\n",
    "      label: An integer from 0 to 9.\n",
    "\n",
    "    Returns: the cross entropy loss, most likely class\n",
    "  \"\"\"\n",
    "     # Reshape the input sample into a column vector\n",
    "  sample = np.array(sample.flatten()).reshape(784, 1)\n",
    "  a = {1: sample} # Initialize the activation dictionary with the input sample\n",
    "  z = {}  # Initialize the dictionary to store the weighted inputs\n",
    "     # Forward pass through the network\n",
    "  for l in range(1, len(weights)):\n",
    "      node_in = a[l]\n",
    "      # Compute the weighted inputs for each layer and apply the activation function\n",
    "      z[l + 1] =  weights[l - 1].dot(node_in) + biases[l - 1]\n",
    "      a[l + 1] = sigmoid(z[l + 1])\n",
    " # Output layer calculations\n",
    "  z[4] = weights[2].dot(a[3]) + biases[2]\n",
    "  a[4] = softmax(z[4])\n",
    "# Convert the true label 'y' into a one-hot encoded vector\n",
    "  y = integer_to_one_hot(y, 10)\n",
    "    # Compute the cross-entropy loss between the predicted and true labels\n",
    "  loss = cross_entropy_loss(y, a[4].flatten()) \n",
    "    # Extract the output of the softmax layer (most likely class probabilities)\n",
    "  one_hot_guess = a[4].flatten()\n",
    "    # Convert the softmax output into a one-hot representation\n",
    "  one_hot_output = one_hot_softmax(one_hot_guess)\n",
    "  return loss, one_hot_output\n",
    "\n",
    "\n",
    "def feed_forward_dataset(x, y):\n",
    "  losses = np.empty(x.shape[0])\n",
    "  one_hot_guesses = np.empty((x.shape[0], 10))\n",
    "\n",
    "  for i in range(len(x)):\n",
    "      losses[i] , one_hot_guesses[i]= feed_forward_sample(x[i] , y[i])\n",
    "\n",
    "  y_one_hot = np.zeros((y.size, 10))\n",
    "  y_one_hot[np.arange(y.size), y] = 1\n",
    "    \n",
    "  correct_guesses = np.sum(y_one_hot * one_hot_guesses)\n",
    "  correct_guess_percent = format((correct_guesses / y.shape[0]) * 100, \".2f\")  \n",
    "\n",
    "  print(\"\\nAverage loss:\", np.round(np.average(losses), decimals=2))\n",
    "  print(\"Accuracy (# of correct guesses):\", correct_guesses, \"/\", y.shape[0], \"(\", correct_guess_percent, \"%)\")\n",
    "\n",
    "def feed_forward_training_data():\n",
    "  print(\"Feeding forward all training data...\")\n",
    "  feed_forward_dataset(x_train, y_train)\n",
    "  print(\"\")\n",
    "\n",
    "def feed_forward_test_data():\n",
    "  print(\"Feeding forward all test data...\")\n",
    "  feed_forward_dataset(x_test, y_test)\n",
    "  print(\"\")\n",
    "\n",
    "feed_forward_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSrlc2VLOi8L"
   },
   "source": [
    "OK, now we will implement the backward pass using backpropagation. We will keep it simple and just do training sample-by-sample (no minibatching, no randomness).\n",
    "\n",
    "Q3: Compute the gradient of all the weights and biases by backpropagating derivatives all the way from the output to the first layer. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init_tri_values function:\n",
    "This function is used to initlaize the change is weights and biases. THe initialization is array of zeros and same size as previously initalized weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_out_layer_delta function: This function caluclate the delta for the output layer. Delta can aslo be said as the diffenriation of y with respect to z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_hidden_delta: This function calculate the delta for hidden layer. This delta depends on the delta from the next layer, weight and z value of the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(0, len(nn_structure) - 1):\n",
    "        tri_W[l] = np.zeros((nn_structure[l+1], nn_structure[l]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l+1], 1))\n",
    "    return tri_W, tri_b\n",
    "\n",
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * dsigmoid(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return w_l.T.dot(delta_plus_1) * dsigmoid(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is a algorithm of backpropogation using gradient descent. we first calcualte the forward pass prediction and based on the prediction and actual label, we calcualte output layer delta and loop it backwards to get all the change in weights and biases and deltas for each layer. After the loop we update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "BLLEsVdcOgzi"
   },
   "outputs": [],
   "source": [
    "def train_one_sample(sample, y, learning_rate=0.003):\n",
    "    # Flatten the input sample and reshape it into a column vector\n",
    "  sample = np.array(sample.flatten()).reshape(784, 1)\n",
    "  a = {1: sample} # Initialize the activation dictionary with the input sample\n",
    "  z = {} # Initialize the dictionary to store the weighted inputs\n",
    "  delta = {}\n",
    "  for l in range(1, len(weights)):\n",
    "      node_in = a[l]\n",
    "       # Calculate the weighted inputs for each layer and apply the activation function\n",
    "      z[l + 1] =  weights[l - 1].dot(node_in) + biases[l - 1]\n",
    "      a[l + 1] = sigmoid(z[l + 1])\n",
    "    # Output layer calculations  \n",
    "  z[4] = weights[2].dot(a[3]) + biases[2]\n",
    "  a[4] = softmax(z[4])\n",
    "  # Convert the true label 'y' into a one-hot encoded vector\n",
    "  y = (integer_to_one_hot(y, 10)).reshape(10,1)\n",
    "  loss = cross_entropy_loss(y, a[4])    # Compute the cross-entropy loss between the predicted and true labels\n",
    "  yHat = a[4]\n",
    "\n",
    "  \n",
    "  # Backward pass\n",
    "  tri_W, tri_b = init_tri_values(layer_sizes) # Initialize the gradients for weights and biases\n",
    "  for l in range(len(layer_sizes), 0, -1): # Iterate through the layers in reverse order\n",
    "      if l == len(layer_sizes):\n",
    "           # Calculate the error at the output layer\n",
    "          delta[l] = calculate_out_layer_delta(y, a[l], z[l])\n",
    "      else:\n",
    "          if l > 1:\n",
    "               # Calculate the error for hidden layers\n",
    "              delta[l] = calculate_hidden_delta(delta[l + 1], weights[l - 1], z[l])\n",
    "               # Accumulate gradients for weights and biases\n",
    "          tri_W[l - 1] += delta[l+1].dot(a[l].T)\n",
    "          tri_b[l - 1] += delta[l + 1]\n",
    " # Update weights and biases using gradient descent\n",
    "  for l in range(len(weights), 1 , -1):\n",
    "      weights[l - 1] += -learning_rate * (tri_W[l - 1])\n",
    "      biases[l - 1] += -learning_rate * (tri_b[l - 1])\n",
    "\n",
    "  return weights, biases\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AXY27pOB9cW"
   },
   "source": [
    "Finally, train for 3 epochs by looping over the entire training dataset 3 times.\n",
    "\n",
    "Q4. Train your model for 3 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code calls the above train example code to train over all training example and get a near optimal weights and biases values which are then test on the test dataset and this porcess continues for 3 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ygk05FcB-rL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding forward all test data...\n",
      "\n",
      "Average loss: 2.36\n",
      "Accuracy (# of correct guesses): 808.0 / 10000 ( 8.08 %)\n",
      "\n",
      "*************************************************\n",
      "\n",
      "Training for one epoch over the training dataset...\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(learning_rate=0.003):\n",
    "  print(\"*************************************************\\n\")\n",
    "  print(\"Training for one epoch over the training dataset...\")\n",
    "   # Loop over the entire training dataset and train each sample\n",
    "  # Q4. Write the training loop over the epoch here.\n",
    "  # ...\n",
    "  for j in range(len(x_train)):\n",
    "     train_one_sample(x_train[j], y_train[j])\n",
    "          \n",
    "  print(\"Finished training.\\n\")\n",
    "# Function to perform forward pass on the test dataset\n",
    "feed_forward_test_data()\n",
    "# Function to perform testing and training\n",
    "def test_and_train():\n",
    "    # Train the model for one epoch\n",
    "  train_one_epoch()\n",
    "    # Perform forward pass on the test data\n",
    "  feed_forward_test_data()\n",
    "# Perform testing and training for a certain number of epochs\n",
    "for i in range(3): \n",
    "  test_and_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Seen above the loss decreases after each epoch and the correct predciton increases after each epoch whihch signifies that our neural network is working properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKzEn_lyCAIe"
   },
   "source": [
    "\n",
    "That's it! \n",
    "\n",
    "Your code is probably very time- and memory-inefficient; that's ok. There is a ton of optimization under the hood in professional deep learning frameworks which we won't get into.\n",
    "\n",
    "If everything is working well, you should be able to raise the accuracy from ~10% to ~70% accuracy after 3 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HG1KMF2-Jxy"
   },
   "source": [
    "\n",
    "**Attributes & Values for Satyrium spini (Spine Hairstreak):**\n",
    "\n",
    "- **Name**: Satyrium spini (Spine Hairstreak) \n",
    "- **Wingspan**: 1.5 to 2 inches (3.8 to 5.1 cm) \n",
    "- **Color**: Dark brown or black with blue and orange markings \n",
    "- **Underside of Wings**: Mottled brown with orange and white markings \n",
    "- **Distinctive Features**: Spine-like projection at the end of the hind wing \n",
    "- **Habitat**: Forests, fields, and gardens \n",
    "- **Nectar Sources**: Variety of flowers \n",
    "- **Attracted to**: Bright, open areas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
